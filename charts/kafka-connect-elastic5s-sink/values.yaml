# Basic info
replicaCount: 1
# secretsRef The secret holding any passwords
secretsRef: "__REQUIRED__"
image:
  repository: registry.hub.docker.com/datamountaineer/kafka-connect-elastic5s
  tag: 0.4.0
  pullPolicy: IfNotPresent

# Resource management
resources:
  limits:
    memory: 512Mi
  requests:
    memory: 256Mi

#javaHeap option
javaHeap: "256M"

# Monitoring
monitoring:
  pipeline: "__REQUIRED__"
  enabled: true
  port: 9102
  path: "/metrics"

# SSL/TLS options can be enabled, some connectors provide SSL support, others the newer TLS
# Set the corresponding value type to true. For SSL persistent volumes or hostPaths will be
# mounted under /connector-extra-config. Set the connectors ssl/tls option paths to be use this
# TLS is done via Secrets, create a secret for containing the certificates (base64 encoded) and
# create a secret for them, adding the name of the secret to the `secretsRef` other wise if using
# Eneco's Landscaper it will do this for you.

# SSL mount path on hosts, should be the base path of any ssl keystore/truststore paths
ssl:
  enabled: false
  # Path to the directory on the hosts
  path: /ssl

  # If persistent volumes should be used for ssl keystore/truststore paths
  persistentVolumes:
    enabled: false
    existingClaim:

# TLS, for those connectors supporting TLS certificates rather than ssl key/truststores
# contents for mount take from config map
tls:
  enabled: false

# lenses 
lensesUser: ""

# Connect values

# clusterName The connect cluster name. This is the consumer group id for the backing topics
clusterName: "__REQUIRED__"
# bootstrapServers A comma separated list of the kafka brokers
bootstrapServers: kafka:9092
# schemaRegistryURL A comman separated list of the Schema registry URL
schemaRegistryURL: "http://schema-registry:8081"
# restPort The rest port of Connect
restPort: 8083
# logLevel The log4j level
logLevel: INFO
# keyConverter The key converter to/from Connects struct
keyConverter: "io.confluent.connect.avro.AvroConverter"
# valueConverter The key converter to/from Connects struct
valueConveter: "io.confluent.connect.avro.AvroConverter"

## Avro schemas, for those connectors, MQTT and JMS source that support Avro converters we
## need the schemas, so we'll mount as a ConfigMap. Set the key value pairs, filename and data
## matching the value option avroSchema. Key is the file name, value is the avro schema contents
#avroSchemaFiles:
#  schema_file_name:  ""

# connectorClass
connectorClass: "com.datamountaineer.streamreactor.connect.elastic5.ElasticSinkConnector"

# applicationId name of the connector
applicationId: "__REQUIRED__"

# maxTasks The number of tasks to spawn
tasksMax: "__REQUIRED__"

# topics to sink
topics: "__REQUIRED__"

# clusterName Name of the elastic search cluster, used in local mode for setting the connection type: STRING importance: HIGH
clusterName: elasticsearch

# maxRetries The maximum number of times to try the write again. type: INT importance: MEDIUM
maxRetries: 20

# writeTimeout The time to wait in millis. Default is 5 minutes. type: INT importance: MEDIUM
writeTimeout: 300000

# errorPolicy Specifies the action to be taken if an error occurs while inserting the data
# There are two available options:
# NOOP - the error is swallowed
# THROW - the error is allowed to propagate.
# RETRY - The exception causes the Connect framework to retry the message. The number of retries is based on
# The error will be logged automatically type: STRING importance: HIGH
errorPolicy: THROW

# xpackPlugins Provide the full class name for all the plugins you want to enable. type: STRING importance: MEDIUM
xpackPlugins: 

# urlPrefix URL connection string prefix type: STRING importance: LOW
urlPrefix: elasticsearch

# enabled Enables the output for how many records have been processed type: BOOLEAN importance: MEDIUM
progressEnabled: true

# xpackSettingsKey XpackSettings key in the secret. Enable xpack security add on by providing this setting type: PASSWORD importance: MEDIUM
xpackSettingsKey: "__REQUIRED__"

# url Url including port for Elastic Search cluster node. type: STRING importance: HIGH
url: localhost:9300

# kcql KCQL expression describing field selection and routes. type: STRING importance: HIGH
kcql: "__REQUIRED__"

# batchSize How many records to process at one time. As records are pulled from Kafka it can be 100k+ which will not be feasible to throw at Elastic search at once type: INT importance: MEDIUM
batchSize: 4000

# useHttp TCP or HTTP. Elastic4s client type to use, http or tcp, default is tcp. type: STRING importance: MEDIUM
useHttp: tcp

