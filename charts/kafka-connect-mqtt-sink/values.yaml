# Basic info
replicaCount: 1

image:
  repository: streamreactor/mqtt
  tag: 1.2.7
  pullPolicy: IfNotPresent

# Resource management
resources:
  limits:
    memory: 512Mi
  requests:
    memory: 256Mi

# javaHeap options    
javaHeap: 256M

# Monitoring
monitoring:
  pipeline: "__REQUIRED__"
  enabled: true
  port: 9102
  path: "/metrics"

podManagementPolicy: OrderedReady

# kafka ssl
# The key and truststores file data are the base64 encoded contents of the files. YOU MUST PROVIDE THE DATA BASE64 encoded
# and added to the kafka secret and mounted into /mnt/connector-secrets
kafka:
  # replicationFactor for connect topics
  replicationFactor: 3
  securityProtocol:
  ssl:
    enabled: false
    trustStoreFileData:
    trustStorePassword:
    keyStoreFileData:
    keyStorePassword:
  sasl:
    enabled: false
    # keyTabData is the contents kerberos keytab file is using kerberos
    keyTabData: |-
             
    # jaasFileData is the contents of the kafka jaas file
    jaasFileData: |-
          
    #GSSAPI, SCRAM or PLAIN
    mechanism: GSSAPI
    # kerberos krb5 contents
    krb5Conf: |-

  bootstrapServers: 
    - name: kafka
      port: 9092
      sslPort: 9093
      saslSslPort: 9094
      saslPlainTextPort: 9095       

schemaRegistries:
  enabled: true
  hosts:
    - host: schema-registry
      protocol: http
      port: 8081
      jmxPort: 9102    

# secretsProvider is either env (k8), vault (hashicorp) or azure (keyvault)
secretsProvider: env

# clusterName The connect cluster name. This is the consumer group id for the backing topics
clusterName: "__REQUIRED__"

# restPort The rest port of Connect
restPort: 8083

# logLevel The log4j level
logLevel: INFO

# keyConverter The key converter to/from Connects struct
keyConverter: "io.confluent.connect.avro.AvroConverter"
keyConverterSchemasEnable: true

# valueConverter The key converter to/from Connects struct
valueConverter: "io.confluent.connect.avro.AvroConverter"
valueConverterSchemasEnable: true

# connectorClass
connectorClass: "com.datamountaineer.streamreactor.connect.mqtt.sink.MqttSinkConnector"

# applicationId name of the connector
applicationId: "__REQUIRED__"

# topics to sink
topics: "__REQUIRED__"

# kcql Contains the Kafka Connect Query Language describing the sourced MQTT source and the target Kafka topics type: STRING importance: HIGH
kcql: "__REQUIRED__"

# hosts Contains the MQTT connection end points. type: STRING importance: HIGH
hosts: "__REQUIRED__"

# clientId Contains the Mqtt session client id type: STRING importance: LOW
clientId: 

# username Contains the Mqtt connection user name type: STRING importance: HIGH
username: 

# password is the Mqtt password which is stored as a secret
password: "__REQUIRED__"

# clean connect.mqtt.clean type: BOOLEAN importance: LOW
clean: true

# keepAlive 
#  The keep alive functionality assures that the connection is still open and both broker and client are connected to
#  the broker during the establishment of the connection. The interval is the longest possible period of time,
#  which broker and client can endure without sending a message.
#      type: INT importance: LOW
keepAlive: 5000

# serviceQuality Specifies the Mqtt quality of service type: INT importance: MEDIUM
serviceQuality: "__REQUIRED__"

# retainedMessages Specifies that the broker should store last retained message type: BOOLEAN importance: MEDIUM
retainedMessages: false

# sslEnabled enabled tls
tlsEnabled: false

# serverCrt is certificate file (server.crt) data to use with the Mqtt connection in the secret type: STRING importance: MEDIUM
serverCrt: |-

# caCert is the CA certificate (ca.crt)  file data to use with the Mqtt connection in the secret type: STRING importance: MEDIUM
caCert: |-

# serverKey Certificate private (server.key) key file data to be mounted in the secret. type: STRING importance: MEDIUM
serverKey: |-

# timeout Provides the time interval to establish the mqtt connection type: INT importance: LOW
timeout: 3000

# pollingTimeout Provides the timeout to poll incoming messages type: INT importance: LOW
pollingTimeout: 1000

# enabled Enables the output for how many records have been processed type: BOOLEAN importance: MEDIUM
progressEnabled: true

# retryInterval The time in milliseconds between retries. type: INT importance: MEDIUM
retryInterval: 60000

# maxRetries The maximum number of times to try the write again. type: INT importance: MEDIUM
maxRetries: 20

# errorPolicy Specifies the action to be taken if an error occurs while inserting the data.
# There are two available options:
# NOOP - the error is swallowed
# THROW - the error is allowed to propagate.
# RETRY - The exception causes the Connect framework to retry the message. The number of retries is based on
# The error will be logged automatically type: STRING importance: HIGH
errorPolicy: THROW

# avroSchemas If the AvroConverter is used you need to provide an avro Schema to be able to read and translate the raw bytes to an avro record. The format is $MQTT_TOPIC=$PATH_TO_AVRO_SCHEMA_FILE type: STRING importance: HIGH
avroSchemas: 

# converterThrowOnError 
#  If set to false the conversion exception will be swallowed and everything carries on BUT the message is lost!!;
#  true will throw the exception.Default is false.
#      type: BOOLEAN importance: HIGH
converterThrowOnError: false


