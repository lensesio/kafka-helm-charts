# Basic info
replicaCount: 1
secretsRef: "__REQUIRED__"
image:
  repository: registry.hub.docker.com/datamountaineer/kafka-connect-bloomberg
  tag: 0.4.0
  pullPolicy: IfNotPresent

# Resource management
resources:
  limits:
    memory: 512Mi
  requests:
    memory: 256Mi
javaHeap: 256M

# Monitoring
monitoring:
  pipeline: "__REQUIRED__"
  enabled: true
  port: 9102
  path: "/metrics"

# SSL/TLS options can be enabled, some connectors provide SSL support, others the newer TLS
# Set the corresponding value type to true. For SSL persistent volumes or hostPaths will be
# mounted under /connector-extra-config. Set the connectors ssl/tls option paths to be use this
# TLS is done via Secrets, create a secret for containing the certificates (base64 encoded) and
# create a secret for them, adding the name of the secret to the `secretsRef` other wise if using
# Eneco's Landscaper it will do this for you.

# SSL mount path on hosts, should be the base path of any ssl keystore/truststore paths
ssl:
  enabled: true
  # Path to the directory on the hosts
  path: /ssl

  # If persistent volumes should be used for ssl keystore/truststore paths
  persistentVolumes:
    enabled: false
    existingClaim:

# TLS, for those connectors supporting TLS certificates rather than ssl key/truststores
# contents for mount take from config map
tls:
  enabled: true

# lenses 
lensesUser: ""

# Connect values
clusterName: "__REQUIRED__"
bootstrapServers: kafka:9092
schemaRegistryURL: "http://schema-registry:8081"
restPort: 8083
logLevel: INFO
keyConverter: "io.confluent.connect.avro.AvroConverter"
valueConveter: "io.confluent.connect.avro.AvroConverter"

javaHeap: "256M"

## Avro schemas, for those connectors, MQTT and JMS source that support Avro converters we
## need the schemas, so we'll mount as a ConfigMap. Set the key value pairs, filename and data
## matching the value option avroSchema. Key is the file name, value is the avro schema contents
#avroSchemaFiles:
#  schema_file_name:  ""

connectorClass: "com.datamountaineer.streamreactor.connect.bloomberg.BloombergSourceConnector"

# applicationId name of the connector
applicationId: "__REQUIRED__"

#maxTasks The number of tasks to spawn
maxTasks: 1

# authenticationMode Optional parameter setting how the authentication should be done. It can be APPLICATION_ONLY or USER_AND_APPLICATION. Follow the Bloomberg API documentation for how to configure this type: STRING importance: LOW
authenticationMode: "__REQUIRED__"

# serverHost The hostname running the bloomberg service type: STRING importance: HIGH
serverHost: "__REQUIRED__"

# subscriptions Provides the list of securities and the fields to subscribe to. Example: "AAPL US Equity:LAST_PRICE,BID,ASK;IBM US Equity:BID,ASK,HIGH,LOW,OPEN" type: STRING importance: HIGH
subscriptions: "__REQUIRED__"

# enabled Enables the output for how many records have been processed type: BOOLEAN importance: MEDIUM
progressEnabled: true

# payloadType Specifies the way the information is serialized and sent over kafka. There are two modes supported: json(default) and avro. type: STRING importance: MEDIUM
payloadType: "__REQUIRED__"

# bufferSize Specifies how big is the queue to hold the updates received from Bloomberg. If the buffer is fullit won't accept new items until it is drained. type: INT importance: MEDIUM
bufferSize: "__REQUIRED__"

# kafkaTopic The name of the kafka topic on which the data from Bloomberg will be sent. type: STRING importance: HIGH
kafkaTopic: "__REQUIRED__"

# serverPort The port on which the bloomberg service runs (8124-is the default) type: INT importance: HIGH
serverPort: "__REQUIRED__"

# serviceUri The Bloomberg service type: Market Data(//blp/mktdata);Reference Data(//blp/refdata) type: STRING importance: HIGH
serviceUri: "__REQUIRED__"

